<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Classfication on Zi Xun</title>
    <link>https://gary0417.github.io/Zi_Xun_Portfolio/tags/classfication/</link>
    <description>Recent content in Classfication on Zi Xun</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Fri, 07 Apr 2023 10:58:08 -0400</lastBuildDate><atom:link href="https://gary0417.github.io/Zi_Xun_Portfolio/tags/classfication/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Project 3: Tomato Disease Classification</title>
      <link>https://gary0417.github.io/Zi_Xun_Portfolio/post/project-3/</link>
      <pubDate>Fri, 07 Apr 2023 10:58:08 -0400</pubDate>
      
      <guid>https://gary0417.github.io/Zi_Xun_Portfolio/post/project-3/</guid>
      <description>üçÖ This project aims to classify diseases in tomato plants using a Convolutional Neural Network (CNN).
üåê A FastAPI web server is built to take in requests for images and return predicted diseases with confidence levels.
üîó Link to GitHub Repository
Model Building The image classification model is built using a CNN architecture implemented with TensorFlow&amp;rsquo;s Keras API. The model architecture consists of multiple Conv2D and MaxPooling2D layers, followed by Flatten and Dropout layers (with a dropout rate of 0.</description>
    </item>
    
    <item>
      <title>Project 2: Naive Bayes Spam Filter</title>
      <link>https://gary0417.github.io/Zi_Xun_Portfolio/post/project-2/</link>
      <pubDate>Sat, 25 Mar 2023 10:58:08 -0400</pubDate>
      
      <guid>https://gary0417.github.io/Zi_Xun_Portfolio/post/project-2/</guid>
      <description>üìß Implemented a spam filter in Python using Naive Bayes from scratch (without the help of pandas and scikit-learn).
üîó Link to GitHub Repository
Dataset SMS Spam Collection Data Set - UCI Machine Learning Repository
Data Preprocessing removed punctuation set all the letters to lower case split the senteces to distinct words randomised the dataset split the data into training data and test data separated spam SMS from ham SMS extracted the SMS from the data (removed the label) extracted all the words that appear in the SMS removed duplicates from the vocabulary Model Building Below are the steps I took to build the model:</description>
    </item>
    
  </channel>
</rss>
