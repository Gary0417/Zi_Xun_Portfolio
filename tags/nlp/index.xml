<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>NLP on Zi Xun</title>
    <link>https://gary0417.github.io/Zi_Xun_Portfolio/tags/nlp/</link>
    <description>Recent content in NLP on Zi Xun</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Mon, 17 Apr 2023 10:58:08 -0400</lastBuildDate><atom:link href="https://gary0417.github.io/Zi_Xun_Portfolio/tags/nlp/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Project 4: Quora Insincere Questions Classification</title>
      <link>https://gary0417.github.io/Zi_Xun_Portfolio/post/project-4/</link>
      <pubDate>Mon, 17 Apr 2023 10:58:08 -0400</pubDate>
      
      <guid>https://gary0417.github.io/Zi_Xun_Portfolio/post/project-4/</guid>
      <description>ðŸ¤” This project aims to classify whether a question asked on Quora is sincere or not using pre-trained NLP text embedding models from TensorFlow Hub.
ðŸ”— Link to GitHub Repository
Data Preprocessing Class imbalance is treated with downsampling technique. The dataset is split into training and validation datasets with an 8:2 ratio. Model Building The text classification model is built using pre-trained NLP text embedding models from Tensorflow Hub. The model architecture consists of the hub layer which includes the pre-trained model from Tensorflow Hub, followed by Dropout layers (with a dropout rate of 0.</description>
    </item>
    
    <item>
      <title>Project 1: Naive Bayes Spam Filter</title>
      <link>https://gary0417.github.io/Zi_Xun_Portfolio/post/project-1/</link>
      <pubDate>Fri, 24 Mar 2023 10:58:08 -0400</pubDate>
      
      <guid>https://gary0417.github.io/Zi_Xun_Portfolio/post/project-1/</guid>
      <description>ðŸ“§ Implemented a spam filter in Python using Naive Bayes from scratch (without the help of pandas and scikit-learn).
ðŸ”— Link to GitHub Repository
Dataset SMS Spam Collection Data Set - UCI Machine Learning Repository
Data Preprocessing removed punctuation set all the letters to lower case split the senteces to distinct words randomised the dataset split the data into training data and test data separated spam SMS from ham SMS extracted the SMS from the data (removed the label) extracted all the words that appear in the SMS removed duplicates from the vocabulary Model Building Below are the steps I took to build the model:</description>
    </item>
    
  </channel>
</rss>
